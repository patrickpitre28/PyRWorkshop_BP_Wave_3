{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting ATM Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to build a classification model with Spark machine learning API (SparkML).\n",
    "\n",
    "The notebook implements the following steps:\n",
    "- Perform data understanding and preparation tasks\n",
    "- Build a model with SparkML API\n",
    "- Evaluate and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytics use case implemented in this notebook is detecting ATM Fraud. While it's a simple use case, it implements all steps from the CRISP-DM methodolody, which is the recommended best practice in data science.\n",
    "![CRISP-DM](https://raw.githubusercontent.com/rosswlewis/ATM_Fraud/master/assets/crisp_dm.png)\n",
    "\n",
    "Use case implementation starts with defining the business problem and identifying the data that can be used to solve the problem. For ATM fraud, we use historical transaction data. We also know which transactions are fraud, which is the critical information for building predictive models. In the next step, we use visual APIs for data understanding and complete some data preparation tasks. In a typical analytics project data preparation will include more steps (for example, formatting data or deriving new variables). \n",
    "\n",
    "Once the data is ready, we can build a predictive model. In our example we show implementaiton of SparkML *Logistic Regression* and *Random Forrest* classification models. Classification is a statistical technique which assigns a \"class\" to each customer record (for our use case \"fraud\" or \"not fraud\"). Classification models use historical data to come up with the logic to predict \"class\", this process is called model training. After the model is created, it's usually evaluated using another data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is generated by Watson Studio \"Insert Spark Dataframe\" function\n",
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-210be2d3-af5e-43b6-9d74-0427ae8ae81a',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': '9FthwkkdExB6wrCH1HafvISYEZEeKAKEKpWfPg9TV2hM'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_b6ce1d66a23747c685affa13595b2acb_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# We renamed the dataframe in the generated code to \"historicalATMFraud\"\n",
    "historicalATMFraud = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load(cos.url('ATM_Data.csv', 'pyrlabtest-donotdelete-pr-ecn1gj2kpu5dli'))\n",
    "historicalATMFraud.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the first step ran successfully (you saw the output), then continue reviewing the notebook and running each code cell step by step. Note that not every cell has a visual output. The cell is still running if you see a * in the brackets next to the cell. \n",
    "\n",
    "If the first step didn't finish successfully, check with the instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The show funtion provides a better view of the data than the take() method\n",
    "historicalATMFraud.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename some columns - they will display better\n",
    "historicalATMFraud = historicalATMFraud.withColumnRenamed('POST_COD_Town','CITY')\n",
    "historicalATMFraud = historicalATMFraud.withColumnRenamed('POST_COD_Region','REGION')\n",
    "historicalATMFraud = historicalATMFraud.withColumnRenamed('Day of Week','DAY')\n",
    "historicalATMFraud = historicalATMFraud.withColumnRenamed('Time of Day','TIME')\n",
    "historicalATMFraud = historicalATMFraud.withColumnRenamed('Time of Day Band','TIME_BAND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we review the data types\n",
    "historicalATMFraud.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used the generated code to read in data, then the data type for all variables is *string*. Some variables, for example, the FRAUD flag, should be an interger. We can convert data types, but if you have many columns, it may be easier to add the *inferSchema* parameter on data load.\n",
    "\n",
    "The modified code to read in data will look like this: \n",
    "\n",
    "**historicalATMFraud = spark.read\\ <BR>\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\ <BR>\n",
    "  .option('header', 'true')\\ <BR>\n",
    "  .option('inferSchema', 'true')\\ <BR>\n",
    "  .load(cos.url('ATM_Data.csv', 'pyrlabtest-donotdelete-pr-ecn1gj2kpu5dli'))**\n",
    "  \n",
    "If you rerun the code to read in data, now **MODEL** and **Time of Day** columns have data type of *int*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "historicalATMFraud.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics may be difficult to read in the row format, so let's display it in the column format\n",
    "historicalATMFraud.describe().show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most variables are non-numeric, we will do additional data understanding later. But the first descriptive statistics, **count**, shows us that we may have some invalid values in several columns. We can confirm the row count by using the count() funtion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalATMFraud.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the describe() method showed us that the following columns have less than 21750 records: **FRAUD, ISSUER_I, MAKE, MODEL, FACILITI, ATM_POSI, INSTITUT, CITY**. There are several ways to deal with missing values. Factors such as number of missing values and the importance of the field for modeling should be taken into consideration. \n",
    "\n",
    "Since we are focusing on building a model, and not on feature selection/engineering, we will use a simple approach:\n",
    "- Drop the MODEL and ISSUER_I field (many missing values and not likely useful for prediciton of fraud)\n",
    "- Drop rows with missing values for the rest of the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns which we will not use for data understanding and modeling\n",
    "historicalATMFraud = historicalATMFraud.drop(\"ATM_ID\",\"MAKE\",\"MODEL\",\"ISSUER_I\")\n",
    "# Display the count of the data frame if we drop the records\n",
    "historicalATMFraud.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since data frames are not mutable, we have to create a new object with null values removed\n",
    "historicalATMFraud.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalATMFraud=historicalATMFraud.na.drop()\n",
    "# Check record count of the new data frame\n",
    "historicalATMFraud.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Understand the Data with Visualizations\n",
    "\n",
    "Visualization is one of the most effective ways to understand data. There are several visualization APIs that can be used. In this sample notebook we will use *PixieDust*, which provides an interactive interface for visualizations. <BR>\n",
    "\n",
    "PixieDust supports Spark data frames, while many other visualization APIs work only with Pandas data frames. <BR>\n",
    "\n",
    "More information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969  <BR>\n",
    "\n",
    "\n",
    "Using the interactive interface, check the number of fradulent and non-fradulent transactions. It's important that the dataset used for modeling is balanced. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "chartsize": "50",
      "handlerId": "barChart",
      "keyFields": "FRAUD",
      "rendererId": "matplotlib",
      "rowCount": "500",
      "title": "Fraudulent Transactions per Hour"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this line once in a notebook\n",
    "#!pip install --upgrade pixiedust\n",
    "\n",
    "\n",
    "from pixiedust.display import *\n",
    "display(historicalATMFraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the Spark pipeline and the Random Forest models\n",
    "\"Pipeline\" is an API in SparkML that's used for building models.\n",
    "Additional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html\n",
    "\n",
    "We are not using all columns for modeling because we can tell that some of them are correlated just by description - *Region* and *City* (we kept *City*), and \n",
    "*Time* and *Time Band* (we kept *Time*)<BR>\n",
    "As mentioned earlier, in a production implementation a data scientist will use statistical and other methods for feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# This code desribes the steps that should be applied to categorical data for modeling. First, we convert String values to an index, and then we encode them.\n",
    "# The steps are recorded in teh \"stages\" object. The indexing/encoding is not applied until we run the Pipeline.fit() method.  \n",
    "categoricalColumns = [\"ATM_POSI\", \"DAY\", \"CITY\", \"SHORTNAM\", \"CARDHOLD\", \"FACILITI\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"FRAUD\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have one numeric column\n",
    "numericCols = [\"TIME\"]\n",
    "\n",
    "# The ML API requires that all features are passed in as a single Vector, which we are creating here\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "# Running this code will apply the string indexing and encoding operations\n",
    "pipelineModel = partialPipeline.fit(historicalATMFraud)\n",
    "preppedDataDF = pipelineModel.transform(historicalATMFraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "# You can review the data that will be used for modeling\n",
    "display(preppedDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data into training and test sets. Set seed for reproducibility\n",
    "(trainingData, testData) = preppedDataDF.randomSplit([0.8, 0.2], seed=100)\n",
    "trainingData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll build a logistic regression model\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView",
      "tableFields": "label,prediction,probability,rawPrediction"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = lrModel.transform(testData)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model - the displayed evaluation is AUC ROC\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will build a Random Forest model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "rf_predictions = rfModel.transform(testData)\n",
    "\n",
    "# Evaluate model\n",
    "rf_evaluator = BinaryClassificationEvaluator()\n",
    "rf_evaluator.evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have finished building and testing a predictive model.** <BR>\n",
    "As shown in the diagram of the CRISP-DM process, a data scientist will usually spend more time on refining the model (testing with different features and algorithms). After the desired accuracy is achieved, a model is deployed in production. We will cover deployment in another lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
