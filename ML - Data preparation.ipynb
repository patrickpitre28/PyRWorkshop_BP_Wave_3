{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data\n",
    "\n",
    "Adopted from https://github.com/addfor/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in scikit-learn, with very few exceptions, is assumed to be stored as a\n",
    "**two-dimensional array**, of size `[n_samples, n_features]`.\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, pandas ``DataFrame``, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Datasets available in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical *scikit-learn* dataset are dictionary-like object that holds all the data and metadata.\n",
    "\n",
    "* **Features** are usually stored in the `.data` field in the form of a 2D array `[n_samples, n_features]`.\n",
    "* **Explanatory variables (targets)** are usually stored in the `.target` field in the form of a 1D array.\n",
    "\n",
    "Scikit-learn makes available a host of datasets for testing learning algorithms:\n",
    "\n",
    "- **Packaged Data:** these small datasets can be downloaded with ``sklearn.datasets.load_*``\n",
    "- **Downloadable Data:** larger datasets that can be fetched from the web with ``sklearn.datasets.fetch_*``\n",
    "- **Generated Data:** can be created with `sklearn.datasets.make_*`\n",
    "\n",
    "Try by yourself:\n",
    "\n",
    "* `datasets.load_<TAB>`\n",
    "* `datasets.fetch_<TAB>`\n",
    "* `datasets.make_<TAB>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets.make_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Example: the \"Iris\" Packaged Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features in the Iris dataset:\n",
    "\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm\n",
    "\n",
    "- Target classes to predict:\n",
    "\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try by yourself one of the following commands where *'d'* is the variable containing the dataset:\n",
    "\n",
    "    print d.keys()           # Structure of the contained data\n",
    "    print d.DESCR            # A complete description of the dataset\n",
    "    print d.data.shape       # [n_samples, n_features]\n",
    "    print d.target.shape     # [n_samples,]\n",
    "    print d.feature_names\n",
    "    datasets.get_data_home() # This is where the datasets are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.keys())\n",
    "print(d.target_names)\n",
    "print(d.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is not required, in many cases it can be easier to manage the data pre-processing with **pandas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.notebook_repr_html = True\n",
    "df = pd.DataFrame(d.data, columns=d.feature_names)\n",
    "df['y'] = d.target\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision',3)\n",
    "df[df.columns[:4]].describe().ix[[1,2,3,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat, y_feat = 2, 3 # Choose the features to plot (0-3)\n",
    "fig = bk.figure(title=None)\n",
    "colors = ['#006CD1', '#26D100', '#D10000']\n",
    "color_series = [ colors[i] for i in df['y'] ]\n",
    "fig.scatter(df[df.columns[x_feat]], df[df.columns[y_feat]], \n",
    "            line_color='black', fill_color=color_series,\n",
    "            radius=0.1)\n",
    "fig.xaxis.axis_label = df.columns[x_feat]\n",
    "fig.yaxis.axis_label = df.columns[y_feat]\n",
    "\n",
    "bk.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas contains some utility functions and plotting functions that can help in previewing the data. In this case we use a `scatter_matrix` pandas plot to plot the four features one versus the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.tools.plotting.scatter_matrix(df[df.columns[:4]], figsize=(10, 10),\n",
    "                                 c=df['y'], diagonal='hist', marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Standardization of datasets is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like *standard normally distributed data*: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center and scale by dividing by the standard deviation.\n",
    "\n",
    "*If the input variables are combined via a distance function* (such as Euclidean distance), standardizing inputs can be crucial. If\n",
    "one input has a range of 0 to 1, while another input has a range of 0 to 1,000,000, then the contribution of the first input to the distance will be swamped by the second input.\n",
    "\n",
    "It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use `sklearn.decomposition.PCA` or `sklearn.decomposition.RandomizedPCA` with `whiten=True` to further remove the linear correlation across features.\n",
    "\n",
    "`scale` and `StandardScaler` work out-of-the-box with 1d arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Standardizing = Mean Removal + Variance Scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`preprocessing.scale` scales each column of the features matric to **mean=0 and std=1**. This is also called **\"STANDARDIZATION\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = np.array([[ 10.,  1., 0.],\n",
    "              [ 20.,  0., 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc_1 = preprocessing.scale(X)\n",
    "print(X)\n",
    "print('\\nScaled Values: ')\n",
    "print(X_sc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`preprocessing.StandardScaler` keeps the values of `.mean_` and `.std_` to allow lately `transform` and `inverse_transform` **mean** and **std** scaling can be controlled independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaler.fit(X)\n",
    "X_sc_2 = scaler.transform(X)\n",
    "print('\\nScaled Values: ')\n",
    "print(X_sc_2)\n",
    "print('\\nStandard Scaler Mean: ', scaler.mean_)\n",
    "print('\\nInverse Transform: ')\n",
    "print(scaler.inverse_transform(X_sc_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using the preprocessing.StandardScaler with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 10.,  1., 0.],\n",
    "              [ 20.,  0., 2.]])\n",
    "df = pd.DataFrame(X)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().ix[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler(copy=False, with_mean=True, with_std=True).fit(df)\n",
    "print('\\nStandard Scaler Mean: ', scaler.mean_)\n",
    "df_sc_2 = pd.DataFrame(scaler.transform(df))\n",
    "df_sc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv =  pd.DataFrame(scaler.inverse_transform(df_sc_2))\n",
    "df_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalizing = Dividing by a Norm of the Vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`preprocessing.normalize` sets thenorm of the vector =1.\n",
    "\n",
    "By default `(axis=1)`, so if it's necessary to normalize the columns  and not the rows, `axis` must be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 10.,  1., 0.],\n",
    "              [ 20.,  0., 2.],\n",
    "              [ 20.,  0., 0.]])\n",
    "X_nrm = preprocessing.normalize(X, norm='l2', axis=0)\n",
    "print('\\nNormalized Values: ')\n",
    "print(X_nrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features extraction (module `sklearn.feature_extraction`) consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Derived Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are obtained by pre-processing the data to generate features that are somehow more informative.  Derived\n",
    "features may be linear or nonlinear combinations of features (such as in Polynomial regression), or may be some more sophisticated transform of the features.  The latter is often used in image processing.\n",
    "\n",
    "For example, [scikit-image](http://scikit-image.org/) provides a variety of feature\n",
    "extractors designed for image data: see the ``skimage.feature`` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 DictVectorizer uses \"one-hot\" encoder for categorical features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_extraction.DictVectorizer` implements what is called one-of-K or “one-hot” coding for categorical (aka nominal, discrete) features.\n",
    "\n",
    "In the following code the method `vec.fit_transform(temp)` returns a sparse matrix that is tranformed to dense by `.toarray()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "t = [{'city': 'Dubai', 'temperature': 33.},\n",
    "     {'city': 'London', 'temperature': 12.},\n",
    "     {'city': 'San Fransisco', 'temperature': 18.},]\n",
    "\n",
    "vec = feature_extraction.DictVectorizer()\n",
    "t_vec = vec.fit_transform(t)\n",
    "df = pd.DataFrame(t_vec.toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Bag of Words representation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. *This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation*. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "`CountVectorizer` implements both tokenization and occurrence counting in a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction.text.CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the x-first document.', 'This is the second second document.',\n",
    "          'And the third third third one.', 'Is this the first document?']\n",
    "vec = feature_extraction.text.CountVectorizer()\n",
    "X = vec.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vec.build_analyzer()\n",
    "analyzer(\"This is the x-first document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Words that were not seen in the training corpus will be completely ignored in future calls to the transform method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec.transform(['Something completely new.']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous corpus we lose the information that the last document is an interrogative form because each word is encoded individually. To preserve the local ordering information we can extract 2-grams of words in addition to the 1-grams (the word themselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_bi = feature_extraction.text.CountVectorizer(min_df=1, ngram_range=(1, 2))\n",
    "X_bi = vec_bi.fit_transform(corpus)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_bi = pd.DataFrame(X_bi.toarray(), columns=vec_bi.get_feature_names())\n",
    "df_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information. For this reason  it is very common to use the `tf–idf` (term-frequency / inverse document-frequency) transform: each row is normalized to have unit euclidean norm. The weights of each feature computed by the fit method call are stored in a model attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ 'aa aa cc dd', 'aa cc',\n",
    "           'aa bb cc ff', \"aa aa\"]\n",
    "tfidf = feature_extraction.text.TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check scikit-learn documentation for additional information on feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
